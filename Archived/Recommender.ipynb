{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/joshuadarcy/Desktop/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Food Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User 1 Liked items\n",
    "u1l = ['spinach','bread','mushrooms','walnut', 'chicken', 'pecans', 'cheeseburger', 'steak']\n",
    "\n",
    "#User 2 Liked items\n",
    "u2l = ['fries','ranch','steak','milk', 'turkey', 'milkshake','fried']\n",
    "\n",
    "#Make them opposite\n",
    "u2d = u1l\n",
    "u1d = u2l\n",
    "\n",
    "#Get word vectors for user1 preferences, create labels\n",
    "u1l = [nlp_model.get_vector(pref) for pref in u1l]\n",
    "u1l_labels = np.ones(len(u1l))\n",
    "\n",
    "#Same for user2\n",
    "u2l = [nlp_model.get_vector(pref) for pref in u2l]\n",
    "u2l_labels = np.ones(len(u2l))\n",
    "\n",
    "#Make them opposite\n",
    "u1d = u2l\n",
    "u2d = u1l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct User 1 Training\n",
    "\n",
    "user1_training_data = np.concatenate((u1l,u1d))\n",
    "user1_training_data = user1_training_data.reshape(-1,1,300)\n",
    "user1_training_labels = np.concatenate((np.zeros(len(u1l)),np.ones(len(u1d))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct User 2 Training\n",
    "\n",
    "user2_training_data = np.concatenate((u2l,u2d))\n",
    "user2_training_data = user2_training_data.reshape(-1,1,300)\n",
    "user2_training_labels = np.concatenate((np.zeros(len(u2l)),np.ones(len(u2d))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Compare Food Preference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and Run for both\n",
    "\n",
    "u1_model = construct_run(user1_training_data, user1_training_labels)\n",
    "u2_model = construct_run(user2_training_data, user2_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1 estimated probability of enjoying steak is 50%\n",
      "User 2 estimated robability of enjoying steak is 49%\n"
     ]
    }
   ],
   "source": [
    "compare_models(u1_model,u2_model,'steak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_run(training_data,labels):\n",
    "    #general model, architecture can be further refined later for opimization\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape = (1,300)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "    ]);\n",
    "    #compile using sparse_cat & adam\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    #train model using 500 epochs, can be changed later. Verbose = 0 for demo.\n",
    "    model.fit(training_data, labels, epochs=500, verbose = 0)\n",
    "    return model\n",
    "\n",
    "def compare_models(model1,model2,keyword): \n",
    "    prediction1 = model1.predict(nlp_model.get_vector(keyword).reshape(1,1,300))[0][0][0]\n",
    "    rounded1 = int(100* round(prediction1, 2))\n",
    "    prediction2 = model2.predict(nlp_model.get_vector(keyword).reshape(1,1,300))[0][0][0]\n",
    "    rounded2 = int(100* round(prediction2, 2))\n",
    "    print(\"User 1 estimated probability of enjoying {0} is {1}%\\nUser 2 estimated robability of enjoying {0} is {2}%\".format(keyword,rounded1,rounded2))\n",
    "    \n",
    "def reshape_scale_data(raw_input,samples): \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(raw_input)\n",
    "    reshaped = scaled_data.reshape(samples,5)\n",
    "    return reshaped, scaler\n",
    "\n",
    "def create_user(preference_mean, preference_sd, distance,step_count,time_of_day,samples): \n",
    "    #create dataset with given standard deviations\n",
    "    params = [(preference_mean,0.2), (preference_sd,.01), (distance, 50), (step_count,400), (time_of_day,100)]\n",
    "    raw_data = np.zeros([samples,len(params)])\n",
    "    labels = np.ones(samples)\n",
    "    for count,param in enumerate(params): \n",
    "        raw_data[:,count] = np.random.normal(param[0], param[1], samples)\n",
    "        counter2 = 0\n",
    "        #if ALL paramters within 1 standard deviation\n",
    "        #respond with 1 \"yes\", else respond with 0 \"no\"\n",
    "\n",
    "        for sample in raw_data[:,count]:\n",
    "            random = np.random.random(1)\n",
    "            if sample > (param[0] + param[1]):\n",
    "                labels[counter2] = 0\n",
    "            if sample < (param[0] - param[1]):\n",
    "                labels[counter2] = 0\n",
    "            if random > .9: \n",
    "                labels[counter2] = 0\n",
    "            counter2 += 1\n",
    "            \n",
    "    data, scaler_un = reshape_scale_data(raw_data,samples)\n",
    "    \n",
    "    return data,labels,scaler_un\n",
    "\n",
    "def naive_model():\n",
    "    state_model = keras.Sequential([\n",
    "        keras.layers.Dense(28, input_shape = (5,)),\n",
    "        keras.layers.Dense(28, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "    ]);\n",
    "    state_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return state_model\n",
    "\n",
    "def create_scenario_vector(menu_item, fp_model, scaler_0, distance, steps, time): \n",
    "    fp = []\n",
    "    for ingredient in menu_item: \n",
    "        prediction = fp_model.predict(nlp_model.get_vector(ingredient).reshape(1,1,300))[0][0][0]\n",
    "        fp.append(prediction)\n",
    "    \n",
    "    fp_avg = sum(fp) / len(fp)\n",
    "    fp_std = .01\n",
    "    input_array = [fp_avg, fp_std, distance, steps, time]\n",
    "    print(input_array)\n",
    "    reshaped = np.array(input_array).reshape(-1,5)\n",
    "    scaled = scaler_0.transform(reshaped)\n",
    "    return scaled\n",
    "\n",
    "def test_input(input_vector, state_model): \n",
    "    estimate = state_model.predict(input_vec)[0][1]\n",
    "    rounded = int(100 * round(estimate,2))\n",
    "    print(\"Estimated chance of user compliance is {}%\".format(rounded))\n",
    "\n",
    "def create_test(preference_mean, preference_sd, distance,step_count,time_of_day,samples): \n",
    "\n",
    "    #create dataset with given standard deviations\n",
    "    params = [(preference_mean,0.5), (preference_sd,.05), (distance, 400), (step_count,400), (time_of_day,300)]\n",
    "    raw_data = np.zeros([samples,len(params)])\n",
    "\n",
    "    for count,param in enumerate(params): \n",
    "        raw_data[:,count] = np.random.normal(param[0], param[1], samples)\n",
    "    return raw_data\n",
    "\n",
    "def return_to_original(scaler,scaled,index_array):\n",
    "    ret = []\n",
    "    for index in index_array: \n",
    "        ret.append(scaler.inverse_transform(scaled[index]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create State Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1data,u1labels,u1scaler = create_user(1,.01,150,1200,1300,2000)\n",
    "u2data,u2labels,u2scaler = create_user(1,.01,100,800,1600,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train State Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 0s 134us/sample - loss: 0.6032 - acc: 0.6585\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 51us/sample - loss: 0.2920 - acc: 0.9105\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 56us/sample - loss: 0.2516 - acc: 0.9105\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2418 - acc: 0.9105\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.2371 - acc: 0.9105\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.2329 - acc: 0.9105\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.2289 - acc: 0.9105\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2255 - acc: 0.9105\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.2223 - acc: 0.9105\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.2186 - acc: 0.9105\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.2148 - acc: 0.9105\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.2111 - acc: 0.9105\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.2071 - acc: 0.9105\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.2030 - acc: 0.9105\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1991 - acc: 0.9110\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1946 - acc: 0.9125\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1906 - acc: 0.9120\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1866 - acc: 0.9160\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1829 - acc: 0.9165\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: 0.1785 - acc: 0.9180\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 68us/sample - loss: 0.1743 - acc: 0.9180\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1709 - acc: 0.9170\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1677 - acc: 0.9205\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1648 - acc: 0.9230\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 0s 51us/sample - loss: 0.1622 - acc: 0.9240\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1602 - acc: 0.9235\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1588 - acc: 0.9235\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 0s 71us/sample - loss: 0.1571 - acc: 0.9250\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 0s 85us/sample - loss: 0.1561 - acc: 0.9250\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1549 - acc: 0.9250\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1537 - acc: 0.9270\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 0s 78us/sample - loss: 0.1518 - acc: 0.9255\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 0s 68us/sample - loss: 0.1521 - acc: 0.9245\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 0s 51us/sample - loss: 0.1509 - acc: 0.9265\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1506 - acc: 0.9260\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: 0.1501 - acc: 0.9260\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 0s 72us/sample - loss: 0.1486 - acc: 0.9265\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1497 - acc: 0.9230\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1479 - acc: 0.9270\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1476 - acc: 0.9255\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1466 - acc: 0.9280\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1481 - acc: 0.9255\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1466 - acc: 0.9260\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1457 - acc: 0.9280\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1466 - acc: 0.9275\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: 0.1456 - acc: 0.9270\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: 0.1458 - acc: 0.9280\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.1449 - acc: 0.9265\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: 0.1455 - acc: 0.9280\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1451 - acc: 0.9285\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1441 - acc: 0.9315\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 0s 65us/sample - loss: 0.1447 - acc: 0.9275\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 0s 77us/sample - loss: 0.1435 - acc: 0.9290\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 0s 65us/sample - loss: 0.1434 - acc: 0.9265\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1450 - acc: 0.9270\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1443 - acc: 0.9280\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1436 - acc: 0.9280\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 0s 53us/sample - loss: 0.1436 - acc: 0.9295\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1436 - acc: 0.9285\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1440 - acc: 0.9285\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1431 - acc: 0.9275\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1427 - acc: 0.9300\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1448 - acc: 0.9280\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1432 - acc: 0.9305\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1423 - acc: 0.9330\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1430 - acc: 0.9280\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1424 - acc: 0.9280\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1421 - acc: 0.9295\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1417 - acc: 0.9290\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1431 - acc: 0.9290\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: 0.1433 - acc: 0.9255\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: 0.1420 - acc: 0.9295\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 0s 76us/sample - loss: 0.1427 - acc: 0.9295\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1422 - acc: 0.9270\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1421 - acc: 0.9280\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1421 - acc: 0.9305\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1425 - acc: 0.9275\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1421 - acc: 0.9305\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 0s 68us/sample - loss: 0.1428 - acc: 0.9275\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1427 - acc: 0.9285\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1418 - acc: 0.9305\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1418 - acc: 0.9285\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1414 - acc: 0.9305\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1430 - acc: 0.9300\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1415 - acc: 0.9290\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 0s 37us/sample - loss: 0.1414 - acc: 0.9290\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1417 - acc: 0.9300\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1440 - acc: 0.9315\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1420 - acc: 0.9315\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1420 - acc: 0.9275\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1412 - acc: 0.9290\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1419 - acc: 0.9295\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1412 - acc: 0.9325\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 0s 67us/sample - loss: 0.1414 - acc: 0.9280\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: 0.1413 - acc: 0.9300\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1426 - acc: 0.9290\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1414 - acc: 0.9280\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1413 - acc: 0.9295\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1416 - acc: 0.9295\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1419 - acc: 0.9300\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1416 - acc: 0.9300\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1415 - acc: 0.9315\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1415 - acc: 0.9275\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1412 - acc: 0.9285\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 0s 51us/sample - loss: 0.1407 - acc: 0.9305\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1412 - acc: 0.9290\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1411 - acc: 0.9300\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 0s 65us/sample - loss: 0.1412 - acc: 0.9295\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1416 - acc: 0.9305\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1416 - acc: 0.9295\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1404 - acc: 0.9305\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1414 - acc: 0.9310\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 0s 75us/sample - loss: 0.1414 - acc: 0.9300\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 0s 53us/sample - loss: 0.1409 - acc: 0.9295\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1406 - acc: 0.9325\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1408 - acc: 0.9320\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1410 - acc: 0.9300\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1413 - acc: 0.9305\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1410 - acc: 0.9305\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 0s 53us/sample - loss: 0.1405 - acc: 0.9320\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 0s 67us/sample - loss: 0.1399 - acc: 0.9295\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1406 - acc: 0.9285\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1405 - acc: 0.9320\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1411 - acc: 0.9290\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1410 - acc: 0.9315\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1405 - acc: 0.9310\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1404 - acc: 0.9325\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1406 - acc: 0.9295\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1414 - acc: 0.9320\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1407 - acc: 0.9330\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1409 - acc: 0.9280\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1416 - acc: 0.9295\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1407 - acc: 0.9295\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1396 - acc: 0.9295\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1408 - acc: 0.9300\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1406 - acc: 0.9300\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1400 - acc: 0.9305\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1403 - acc: 0.9325\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1409 - acc: 0.9295\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1405 - acc: 0.9305\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1400 - acc: 0.9295\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1407 - acc: 0.9285\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1405 - acc: 0.9320\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1401 - acc: 0.9310\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1397 - acc: 0.9305\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1400 - acc: 0.9310\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1396 - acc: 0.9310\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1401 - acc: 0.9315\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1404 - acc: 0.9305\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1402 - acc: 0.9310\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1396 - acc: 0.9315\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1403 - acc: 0.9320\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1398 - acc: 0.9305\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1397 - acc: 0.9305\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1399 - acc: 0.9325\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1402 - acc: 0.9300\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1400 - acc: 0.9300\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1400 - acc: 0.9300\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1393 - acc: 0.9325\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1391 - acc: 0.9310\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1394 - acc: 0.9305\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1391 - acc: 0.9305\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1402 - acc: 0.9320\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1387 - acc: 0.9305\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1398 - acc: 0.9300\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1415 - acc: 0.9315\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1395 - acc: 0.9310\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1401 - acc: 0.9295\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1393 - acc: 0.9315\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1394 - acc: 0.9305\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1401 - acc: 0.9300\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1399 - acc: 0.9285\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1395 - acc: 0.9330\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1389 - acc: 0.9290\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1395 - acc: 0.9325\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1395 - acc: 0.9300\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1394 - acc: 0.9315\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1399 - acc: 0.9315\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1392 - acc: 0.9285\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1397 - acc: 0.9335\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1387 - acc: 0.9280\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1396 - acc: 0.9290\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1402 - acc: 0.9290\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1384 - acc: 0.9310\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1377 - acc: 0.9325\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1392 - acc: 0.9315\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1390 - acc: 0.9300\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1384 - acc: 0.9335\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1386 - acc: 0.9295\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1384 - acc: 0.9330\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1385 - acc: 0.9285\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1387 - acc: 0.9320\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1389 - acc: 0.9295\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1385 - acc: 0.9295\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1379 - acc: 0.9335\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1380 - acc: 0.9275\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1404 - acc: 0.9275\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1387 - acc: 0.9310\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1388 - acc: 0.9305\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1391 - acc: 0.9285\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 0s 120us/sample - loss: 0.5049 - acc: 0.7755\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.2534 - acc: 0.9195\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.2239 - acc: 0.9195\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2178 - acc: 0.9195\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.2132 - acc: 0.9195\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2094 - acc: 0.9195\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.2064 - acc: 0.9195\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.2035 - acc: 0.9195\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.2006 - acc: 0.9195\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1976 - acc: 0.9195\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1948 - acc: 0.9195\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1923 - acc: 0.9195\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1893 - acc: 0.9195\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1870 - acc: 0.9195\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1846 - acc: 0.9200\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1825 - acc: 0.9200\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1803 - acc: 0.9200\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1779 - acc: 0.9195\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1755 - acc: 0.9200\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1731 - acc: 0.9205\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1715 - acc: 0.9200\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1689 - acc: 0.9200\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1662 - acc: 0.9195\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1634 - acc: 0.9195\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1609 - acc: 0.9175\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1582 - acc: 0.9195\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1559 - acc: 0.9205\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1530 - acc: 0.9265\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1510 - acc: 0.9240\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1497 - acc: 0.9265\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1475 - acc: 0.9270\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1463 - acc: 0.9285\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1448 - acc: 0.9265\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1433 - acc: 0.9295\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1424 - acc: 0.9285\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1422 - acc: 0.9275\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1414 - acc: 0.9305\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1410 - acc: 0.9270\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1405 - acc: 0.9290\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1394 - acc: 0.9295\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1387 - acc: 0.9295\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1386 - acc: 0.9320\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1384 - acc: 0.9305\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1378 - acc: 0.9335\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1378 - acc: 0.9280\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1366 - acc: 0.9330\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1364 - acc: 0.9310\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1363 - acc: 0.9315\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1365 - acc: 0.9305\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1356 - acc: 0.9345\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1354 - acc: 0.9310\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1363 - acc: 0.9315\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1351 - acc: 0.9320\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1348 - acc: 0.9330\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1348 - acc: 0.9325\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1350 - acc: 0.9375\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1349 - acc: 0.9355\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1336 - acc: 0.9325\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1342 - acc: 0.9330\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1345 - acc: 0.9350\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1349 - acc: 0.9340\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1332 - acc: 0.9335\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1341 - acc: 0.9320\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1335 - acc: 0.9335\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1327 - acc: 0.9375\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1345 - acc: 0.9315\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1337 - acc: 0.9330\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1337 - acc: 0.9365\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1331 - acc: 0.9340\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1326 - acc: 0.9350\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1336 - acc: 0.9350\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1331 - acc: 0.9350\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1322 - acc: 0.9370\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1325 - acc: 0.9335\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1328 - acc: 0.9350\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1321 - acc: 0.9360\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1323 - acc: 0.9355\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1319 - acc: 0.9330\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1321 - acc: 0.9370\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1322 - acc: 0.9370\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1327 - acc: 0.9355\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1322 - acc: 0.9315\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1334 - acc: 0.9360\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1322 - acc: 0.9365\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1315 - acc: 0.9335\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1315 - acc: 0.9370\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1316 - acc: 0.9370\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1313 - acc: 0.9395\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1322 - acc: 0.9345\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1311 - acc: 0.9355\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1311 - acc: 0.9350\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1309 - acc: 0.9355\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1318 - acc: 0.9345\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1321 - acc: 0.9350\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1317 - acc: 0.9365\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1314 - acc: 0.9330\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1307 - acc: 0.9370\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1313 - acc: 0.9355\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1313 - acc: 0.9375\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1313 - acc: 0.9355\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1311 - acc: 0.9350\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1320 - acc: 0.9365\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1305 - acc: 0.9365\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1310 - acc: 0.9365\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1308 - acc: 0.9385\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1309 - acc: 0.9345\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1306 - acc: 0.9380\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1300 - acc: 0.9365\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1306 - acc: 0.9380\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1309 - acc: 0.9380\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1307 - acc: 0.9365\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 0s 38us/sample - loss: 0.1299 - acc: 0.9370\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1306 - acc: 0.9345\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1310 - acc: 0.9385\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1304 - acc: 0.9355\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1300 - acc: 0.9365\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1302 - acc: 0.9355\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1305 - acc: 0.9350\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1307 - acc: 0.9375\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1297 - acc: 0.9375\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1304 - acc: 0.9355\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1301 - acc: 0.9345\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1303 - acc: 0.9345\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1303 - acc: 0.9385\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1305 - acc: 0.9390\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1304 - acc: 0.9370\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1302 - acc: 0.9360\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1307 - acc: 0.9350\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1306 - acc: 0.9360\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1306 - acc: 0.9350\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1311 - acc: 0.9360\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1294 - acc: 0.9355\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1302 - acc: 0.9350\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.1296 - acc: 0.9345\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1299 - acc: 0.9360\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1294 - acc: 0.9360\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1296 - acc: 0.9345\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1303 - acc: 0.9385\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1315 - acc: 0.9360\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1314 - acc: 0.9365\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1297 - acc: 0.9365\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1297 - acc: 0.9355\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1310 - acc: 0.9360\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.1221 - acc: 0.942 - 0s 43us/sample - loss: 0.1296 - acc: 0.9365\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1301 - acc: 0.9350\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1300 - acc: 0.9355\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1296 - acc: 0.9355\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1300 - acc: 0.9350\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1301 - acc: 0.9375\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 0s 39us/sample - loss: 0.1292 - acc: 0.9380\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1291 - acc: 0.9355\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1299 - acc: 0.9350\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1291 - acc: 0.9355\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1294 - acc: 0.9350\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1314 - acc: 0.9340\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 0s 51us/sample - loss: 0.1298 - acc: 0.9355\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1287 - acc: 0.9370\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1292 - acc: 0.9370\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1297 - acc: 0.9360\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1306 - acc: 0.9385\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1295 - acc: 0.9360\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1299 - acc: 0.9330\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1288 - acc: 0.9400\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1304 - acc: 0.9345\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1293 - acc: 0.9380\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1296 - acc: 0.9350\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1298 - acc: 0.9340\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1301 - acc: 0.9360\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 0s 51us/sample - loss: 0.1304 - acc: 0.9355\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1295 - acc: 0.9375\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1289 - acc: 0.9370\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1297 - acc: 0.9350\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1291 - acc: 0.9385\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1285 - acc: 0.9370\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.1291 - acc: 0.9355\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 0s 55us/sample - loss: 0.1288 - acc: 0.9355\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1289 - acc: 0.9370\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1294 - acc: 0.9385\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 0s 50us/sample - loss: 0.1286 - acc: 0.9370\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.1288 - acc: 0.9370\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1289 - acc: 0.9335\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1290 - acc: 0.9350\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1290 - acc: 0.9370\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 0s 56us/sample - loss: 0.1292 - acc: 0.9380\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1293 - acc: 0.9360\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.1224 - acc: 0.934 - 0s 46us/sample - loss: 0.1285 - acc: 0.9325\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1290 - acc: 0.9355\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1284 - acc: 0.9365\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 0s 49us/sample - loss: 0.1292 - acc: 0.9375\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1292 - acc: 0.9345\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.1292 - acc: 0.9375\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1293 - acc: 0.9360\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1287 - acc: 0.9345\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.1294 - acc: 0.9355\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.1289 - acc: 0.9340\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.1282 - acc: 0.9335\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1285 - acc: 0.9360\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.1287 - acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.1304 - acc: 0.9325\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.1289 - acc: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x266a16650>"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1_state_model = naive_model()\n",
    "u1_state_model.fit(u1data, u1labels, epochs=200, verbose = 1)\n",
    "\n",
    "u2_state_model = naive_model()\n",
    "u2_state_model.fit(u2data, u2labels, epochs=200, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Individual Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999706745147705, 0.01, 150, 1200, 1300]\n",
      "Estimated chance of user compliance is 98%\n"
     ]
    }
   ],
   "source": [
    "food = ['kale','bread']\n",
    "input_vec = create_scenario_vector(food,u1_model,u1scaler,150,1200,1300)\n",
    "test_input(input_vec, u1_state_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999551177024841, 0.01, 100, 800, 1600]\n",
      "Estimated chance of user compliance is 98%\n"
     ]
    }
   ],
   "source": [
    "food = ['fries','turkey']\n",
    "input_vec = create_scenario_vector(food,u2_model,u2scaler, 100,800,1600)\n",
    "test_input(input_vec, u2_state_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrarily Create 1 Million Scenarios, Choose Top 20 for Each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_test(1,.01, 125, 1000, 1450, 1000000)\n",
    "scaled_u1 = u1scaler.transform(test_data)\n",
    "scaled_u2 = u2scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1 = np.array(u1_state_model.predict(scaled_u1)).T[1]\n",
    "array_2 = np.array(u2_state_model.predict(scaled_u2)).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = np.argsort(-array_1)[0:20]\n",
    "index2 = np.argsort(-array_2)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_20_u1 = return_to_original(u1scaler,scaled_u1,index1)\n",
    "best_20_u2 = return_to_original(u2scaler,scaled_u2,index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((best_20_u1, best_20_u2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "label_array = np.concatenate((np.ones(20),2*np.ones(20)))\n",
    "labels = pd.DataFrame(label_array,columns = ['Label'])\n",
    "finalDf = pd.concat([principalDf, labels], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('PCA (2)', fontsize = 20)\n",
    "targets = [1.0, 2.0]\n",
    "colors = ['r', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['Label'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "\n",
    "stringed = ['User {}'.format(target)[0:6] for target in targets]\n",
    "ax.legend(stringed)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041435394"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_model.similarity('food','Alaska')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algo for Christina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steak\n",
      "pear\n",
      "apple\n",
      "crab\n"
     ]
    }
   ],
   "source": [
    "test_word = ['steak','Alaska','basketball','pear','tabletop', 'fingernail', 'apple', 'thumb', 'snow', 'crab']\n",
    "\n",
    "for word in test_word: \n",
    "    if nlp_model.similarity('edible',word) > .1 and nlp_model.similarity('food',word) > .15:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
